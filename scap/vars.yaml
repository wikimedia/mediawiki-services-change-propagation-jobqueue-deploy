# log-related variables
log_name: cpjobqueue
log_file: /tmp/cpjq.log
rsyslog_port: 10514
# metrics
metrics_name: cpjobqueue
metrics_host: localhost
metrics_port: 8252
# service
name: cpjobqueue
port: 7200
proxy:
site: datacenter1
broker_list: localhost:9092
jobrunner_host: http://localhost
videoscaler_host: http://localhost
redis_path: /var/run/redis.sock
redis_pass:
env: production
kafka_max_bytes: 4194304
kafka_compression_codec: snappy
# The default concurrency is used whenever no specific settings are set for a job.
concurrency: 10
# All the jobs listed below get their own rule, which transfers to
# their own processing unit - each type of job listed explicitly is processed
# by a separate worker in change-prop.
high_traffic_jobs_config:
  categoryMembershipChange:
    concurrency: 200
  # CNDPurge is quite low-volume, but it uses delayed execution,
  # so avoid putting it together with other low-volume jobs so that it doesn't
  # block execution for others.
  cdnPurge:
    concurrency: 5
  ORESFetchScoreJob:
    concurrency: 20
  # RecordLinks is normally low-volume, but could have big spikes
  # when maintenance scripts are run. Elevated concurrency
  RecordLintJob:
    concurrency: 50
    consumer_batch_size: 10
  wikibase-addUsagesForPage:
    concurrency: 5
  constraintsRunCheck:
    concurrency: 30
  fetchGoogleCloudVisionAnnotations:
    concurrency: 10
    # All the jobs of this kind are delayed exactly 48 hours, so we don't want
    # the reenqueue feature to kick in.
    reenqueue_delay: 259200
  # For cirrus search jobs the retries are built into the job itself,
  # so disable the retries by change-prop. We need special rules for cirrus
  # jobs because they need special configuration.
  cirrusSearchCheckerJob:
    disable_delayed_execution: true #T198462
    retry_limit: 0
    concurrency: 20
  cirrusSearchDeleteArchive:
    retry_limit: 0
    concurrency: 5
  cirrusSearchDeletePages:
    retry_limit: 0
    concurrency: 5
  cirrusSearchIncomingLinkCount:
    retry_limit: 0
    concurrency: 15
  cirrusSearchLinksUpdate:
    retry_limit: 0
    concurrency: 300
  cirrusSearchLinksUpdatePrioritized:
    retry_limit: 0
    concurrency: 150
  cirrusSearchOtherIndex:
    retry_limit: 0
    concurrency: 5
# Videoscaler jobs point to a different LVS, so they need special treatment
# as well - thus special rules.
videoscaler_jobs_config:
  webVideoTranscode:
    timeout: 86400000
    concurrency: 50
  webVideoTranscodePrioritized:
    concurrency: 30
    timeout: 86400000
# Some jobs require partitioning according to MariaDB shards.
partitioned_jobs_config:
  refreshLinks:
    # Partition jobs by mediawiki database cluster (s1, s2, etc.)
    partitioner_kind: mediawiki_database
    # This is the concurrency for the partitioner
    # itself, it's does not actually touch Mediawiki, only re-enqueues the
    # jobs according to proper partitions
    partitioner_concurrency: 200
    partition:
      # This is the concurrency of the individual partitions, so overall concurrency
      # is 8 * 20 = 160
      concurrency: 20
      # Abandon jobs which root job is more than 1 week long
      root_claim_ttl: 604800
  htmlCacheUpdate:
    # Partition jobs by mediawiki database cluster (s1, s2, etc.)
    partitioner_kind: mediawiki_database
    partitioner_concurrency: 50
    partition:
      # This is the concurrency of the individual partitions, so overall concurrency
      # is 8 * 4 = 32
      # The load of htmlCacheUpdate is uneven across partitions, so we are using a bit
      # higher overall concurrency then needed.
      concurrency: 4
      # Abandon jobs which root job is more than 1 week long
      root_claim_ttl: 604800
  cirrusSearchElasticaWrite:
    partitioner_kind: cirrussearch_cluster
    partitioner_concurrency: 50
    partition:
      # This is the concurrency of the individual partitions, so overall concurrency
      # is 100 * 3 = 300
      concurrency: 100
      reenqueue_delay: 3600
      retry_limit: 0
# All the jobs not explicitly specified in the config are combined into the
# `low_traffic_jobs` rule, so they share a worker. The low_traffic_concurrency
# is shared between all the jobs other then the exceptions listed above.
#
# Most of the topics are empty most of the time, so a lot of slots are just waiting
# for the `consume` method to timeout and do nothing.
# So a significantly higher concurrency is needed to account for that.
low_traffic_concurrency: 50
